# GuppShupp AI - AI Companion Demo

A minimal Python Streamlit application demonstrating an AI companion with personality, memory, and LLM integration.

## ğŸ—ï¸ Architecture

This project follows a clean, modular architecture with the following components:

```
guppshupp-ai/
â”œâ”€â”€ app.py                 # Main Streamlit application entry point
â”œâ”€â”€ llm/                   # LLM client wrapper module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ client.py          # Ollama-based LLM client
â”œâ”€â”€ memory/                # Conversation memory module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ conversation.py    # Memory extraction and management
â”œâ”€â”€ personality/           # Response tone engine module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ engine.py          # Personality and tone configuration
â”œâ”€â”€ data/                  # Sample chat data
â”‚   â”œâ”€â”€ sample_chat.json   # Example conversations
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ requirements.txt       # Project dependencies
â””â”€â”€ README.md             # This file
```

### Module Descriptions

#### ğŸ¤– LLM Module (`llm/`)
- **Purpose**: Provides a wrapper for Ollama-based language model interactions
- **Key Features**:
  - Ollama API integration with configurable endpoint
  - Error handling and connection status checking
  - Context-aware prompt generation
  - No hardcoded API keys (uses environment variables)

#### ğŸ§  Memory Module (`memory/`)
- **Purpose**: Manages conversation history and memory extraction
- **Key Features**:
  - Maintains conversation history with configurable limits
  - Extracts conversation context for LLM prompts
  - Topic extraction from conversations
  - Conversation summarization

#### ğŸ­ Personality Module (`personality/`)
- **Purpose**: Manages response tone and AI personality traits
- **Key Features**:
  - Multiple personality tones (friendly, professional, casual, empathetic, enthusiastic)
  - Dynamic tone application to responses
  - Customizable system prompts
  - Runtime personality switching

#### ğŸ“Š Data Module (`data/`)
- **Purpose**: Contains sample conversation data
- **Key Features**:
  - Example chat conversations
  - Demonstrates conversation format
  - Can be loaded for testing

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8 or higher
- (Optional) [Ollama](https://ollama.ai/) installed locally for full LLM functionality

### Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/Vinaytandle/guppshupp-ai.git
   cd guppshupp-ai
   ```

2. **Create a virtual environment** (recommended):
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment** (optional):
   Create a `.env` file in the project root for custom configurations:
   ```env
   OLLAMA_BASE_URL=http://localhost:11434
   ```

### Running the Application

1. **Start the Streamlit app**:
   ```bash
   streamlit run app.py
   ```

2. **Access the application**:
   Open your browser and navigate to `http://localhost:8501`

### Running with Ollama (Optional)

For full AI functionality, install and run Ollama:

1. **Install Ollama**:
   Follow instructions at [ollama.ai](https://ollama.ai/)

2. **Pull a model**:
   ```bash
   ollama pull llama2
   ```

3. **Ensure Ollama is running**:
   ```bash
   ollama serve
   ```

4. **Run the Streamlit app**:
   The app will automatically detect and connect to Ollama

### Demo Mode

If Ollama is not available, the app runs in demo mode with placeholder responses. You can still:
- Test the UI and conversation flow
- Try different personality tones
- Load and view sample conversations
- Explore memory and context features

## ğŸ¯ Features

- **ğŸ¨ Multiple Personality Tones**: Choose from friendly, professional, casual, empathetic, or enthusiastic
- **ğŸ’­ Conversation Memory**: Maintains context across messages
- **ğŸ“ Topic Extraction**: Automatically identifies conversation topics
- **ğŸ”„ Sample Data Loading**: Load example conversations to test the app
- **ğŸ›ï¸ Real-time Configuration**: Change settings without restarting
- **ğŸ”Œ Modular Design**: Easy to extend and customize

## ğŸ› ï¸ Development

### Project Structure

The project uses a modular structure where each component is independent and can be tested separately:

- **app.py**: Orchestrates all modules and provides the UI
- **llm/**: Handles all LLM-related operations
- **memory/**: Manages conversation state and history
- **personality/**: Controls response tone and style
- **data/**: Stores sample and configuration data

### Configuration

No API keys are hardcoded. Configuration is managed through:
- Environment variables (`.env` file)
- Streamlit session state
- Module initialization parameters

### Extending the Application

To add new features:

1. **Add a new personality tone**: Edit `personality/engine.py` and add to `PersonalityTone` enum
2. **Integrate a different LLM**: Implement a new client in `llm/` following the same interface
3. **Enhance memory**: Extend `ConversationMemory` class with new extraction methods
4. **Add new UI features**: Modify `app.py` following Streamlit conventions

## ğŸ“¦ Dependencies

- **streamlit**: Web application framework
- **requests**: HTTP library for API calls
- **python-dotenv**: Environment variable management

See `requirements.txt` for specific versions.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit issues or pull requests.

## ğŸ“„ License

This project is open source and available for educational and demonstration purposes.

## ğŸ™ Acknowledgments

- Built with [Streamlit](https://streamlit.io/)
- LLM integration powered by [Ollama](https://ollama.ai/)

---

**Note**: This is a minimal demo project designed to showcase architecture and modular design. For production use, consider adding authentication, database persistence, and more robust error handling.